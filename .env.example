# Ollama Model Configuration
# Copy this file to .env and update with your actual Ollama server details

# Ollama server base URL (include port if not default)
OLLAMA_BASE_URL=http://localhost:11434

# Model name to use (must be available in your Ollama instance)
OLLAMA_MODEL=qwen2.5:7b

# Example configurations for different models:
# OLLAMA_MODEL=deepseek-r1:70b    # Large reasoning model
# OLLAMA_MODEL=gpt-oss:20b        # GPT-style model
# OLLAMA_MODEL=llama3.2:3b        # Fast, lightweight model
